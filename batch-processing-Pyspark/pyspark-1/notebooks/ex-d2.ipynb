{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ea709e-18f3-4b31-ac0a-7964ca1d8f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 15.0\n"
     ]
    }
   ],
   "source": [
    "##1 Create an RDD from [5, 10, 15, 20, 25] and compute the average value.\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize Spark context\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Create an RDD from a Python list\n",
    "numbers = [5, 10, 15, 20, 25]\n",
    "rdd = sc.parallelize(numbers)  # parallelize converts list to RDD\n",
    "\n",
    "# Compute the sum of all numbers using reduce\n",
    "total_sum = rdd.reduce(lambda x, y: x + y)  # reduce applies the lambda cumulatively\n",
    "\n",
    "# Count the total number of elements in the RDD\n",
    "count = rdd.count() \n",
    "\n",
    "# Compute the average\n",
    "average = total_sum / count\n",
    "\n",
    "# Print the result\n",
    "print(\"Average:\", average)  # Output: Average: 15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82019e52-84f5-4369-bf5f-f29fa1efe655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct values: 5\n"
     ]
    }
   ],
   "source": [
    "##2 Create an RDD from [1, 2, 2, 3, 4, 4, 5] and count distinct numbers.\n",
    "\n",
    "\n",
    "# Create an RDD from a Python list\n",
    "numbers = [1, 2, 2, 3, 4, 4, 5]\n",
    "rdd = sc.parallelize(numbers)  # Convert list to RDD\n",
    "\n",
    "# Get distinct values and count them\n",
    "distinct_count = rdd.distinct().count()\n",
    "\n",
    "# distinct() removes duplicates\n",
    "# count() returns total unique elements\n",
    "\n",
    "# Print the result\n",
    "print(\"Number of distinct values:\", distinct_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "380e0c04-5144-496e-8de1-97900524285f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number: 10\n"
     ]
    }
   ],
   "source": [
    "##3 Create an RDD from [3, 8, 2, 10, 6] and find the maximum number.\n",
    "\n",
    "\n",
    "# Create an RDD from a Python list\n",
    "numbers = [3, 8, 2, 10, 6]\n",
    "rdd = sc.parallelize(numbers)  # Convert list to RDD\n",
    "\n",
    "# Find the maximum value using reduce\n",
    "max_num = rdd.reduce(lambda a, b: a if a > b else b)  \n",
    "# Compare elements pairwise, keep the larger one\n",
    "\n",
    "# Print the maximum value\n",
    "print(\"Maximum number:\", max_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a9b3f0-472a-4ca2-9293-97d90f42dde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total new cases for Afghanistan: 235214.0\n"
     ]
    }
   ],
   "source": [
    "##4 Load covid-dataset/covid-data.csv, filter for Afghanistan (iso_code = 'AFG'), \n",
    "    # and compute total new_cases.\n",
    "\n",
    "\n",
    "# Load CSV file into an RDD (each line is a string)\n",
    "rdd = sc.textFile(\"covid-dataset/covid-data.csv\")  \n",
    "\n",
    "# Get the header row\n",
    "header = rdd.first()  \n",
    "\n",
    "# Filter out header and keep only Afghanistan data (iso_code == 'AFG')\n",
    "afg_data = rdd.filter(lambda row: row != header and row.startswith(\"AFG\"))  \n",
    "\n",
    "# Extract 'new_cases' column (6th column) and convert to float\n",
    "new_cases = afg_data.map(lambda row: float(row.split(',')[5] or 0.0))  \n",
    "\n",
    "# Compute total new cases\n",
    "total_cases = new_cases.sum()  \n",
    "\n",
    "# Display total new cases\n",
    "print(f\"Total new cases for Afghanistan: {total_cases}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1054b3bc-3a46-4c19-959b-6157f788e5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of factorials: 153\n"
     ]
    }
   ],
   "source": [
    "##5 Create an RDD from [1, 2, 3, 4, 5], compute factorials, and sum them.\n",
    "\n",
    "import math\n",
    "\n",
    "# Create an RDD from a Python list\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(numbers)  # parallelize converts list to RDD\n",
    "\n",
    "# Compute factorial of each number and sum them\n",
    "factorial_sum = (\n",
    "    rdd\n",
    "    .map(lambda x: math.factorial(x))  # map: compute factorial of each element\n",
    "    .reduce(lambda a, b: a + b)       # reduce: sum all factorials\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "print(\"Sum of factorials:\", factorial_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "639dbe56-9c01-45ca-b942-d057da1a8e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of cubes of odd numbers: 496\n"
     ]
    }
   ],
   "source": [
    "##6 Create an RDD from [1, 2, 3, 4, 5, 6, 7], filter odd numbers, cube them, and compute the sum.\n",
    "\n",
    "\n",
    "numbers = [1, 2, 3, 4, 5, 6, 7]\n",
    "rdd = sc.parallelize(numbers)\n",
    "\n",
    "# Keep only odd numbers, cube them, and sum the results\n",
    "result = (\n",
    "    rdd\n",
    "    .filter(lambda x: x % 2 != 0)   # Odd numbers only\n",
    "    .map(lambda x: x**3)            # Cube each\n",
    "    .reduce(lambda a, b: a + b)     # Sum all cubes\n",
    ")\n",
    "\n",
    "print(\"Sum of cubes of odd numbers:\", result) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d979b72-0df1-4b23-bc7a-8fdee9748841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 26525), ('Europe', 91031), ('Africa', 95419), ('North America', 68638), ('South America', 23440), ('Asia', 84199), ('Oceania', 40183)]\n"
     ]
    }
   ],
   "source": [
    "##7 Load covid-dataset/covid-data.csv, group by continent, and count records per continent.\n",
    "\n",
    "\n",
    "rdd = sc.textFile(\"covid-dataset/covid-data.csv\")  # Load CSV\n",
    "header = rdd.first()  # Get header\n",
    "counts = (rdd\n",
    "          .filter(lambda row: row != header)              # Skip header\n",
    "          .map(lambda row: (row.split(',')[1], 1))        # (continent, 1)\n",
    "          .reduceByKey(lambda a, b: a + b))               # Sum per continent\n",
    "\n",
    "print(counts.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dfe6d72-d71c-4d07-9694-9ad1a7ddef3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('S2', ('Bob', 90)), ('S1', ('Alice', 85))]\n"
     ]
    }
   ],
   "source": [
    "##8 Create RDDs: [('S1', 'Alice'), ('S2', 'Bob'), ('S3', 'Charlie')] and [('S1', 85), ('S2', 90),\n",
    "    #('S4', 95)]. Join on student ID and collect results.\n",
    "\n",
    "\n",
    "students = sc.parallelize([('S1','Alice'), ('S2','Bob'), ('S3','Charlie')])  # RDD of student IDs & names\n",
    "grades = sc.parallelize([('S1',85), ('S2',90), ('S4',95)])                 # RDD of student IDs & grades\n",
    "\n",
    "joined_rdd = students.join(grades)  # Join on student ID (key)\n",
    "\n",
    "print(joined_rdd.collect())  # Collect joined RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b1ab714-efa3-448d-b998-fe5077aabcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average new deaths for Brazil: 22408.555053763415\n"
     ]
    }
   ],
   "source": [
    "##9 Load covid-dataset/covid-data.csv, filter for Brazil (iso_code = 'BRA'), cache, \n",
    "    # and compute average new_deaths.\n",
    "\n",
    "\n",
    "rdd = sc.textFile(\"covid-dataset/covid-data.csv\")  # Load CSV\n",
    "header = rdd.first()                               # Get header\n",
    "bra_data = rdd.filter(lambda row: row != header and row.startswith(\"BRA\")).cache()  # Filter Brazil, cache\n",
    "\n",
    "new_deaths = bra_data.map(lambda row: float(row.split(',')[6] or 0.0))  # Extract new_deaths column\n",
    "count = new_deaths.count()                                             # Count rows\n",
    "total = new_deaths.sum()                                               # Sum of new_deaths\n",
    "average = total / count if count > 0 else 0.0                          # Compute average\n",
    "\n",
    "print(f\"Average new deaths for Brazil: {average}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "694a6632-2cf5-49fc-b8fa-b7e7be2ccdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('apple', 8), ('orange', 8), ('banana', 2)]\n"
     ]
    }
   ],
   "source": [
    "##10 Create an RDD from [('apple', 5), ('banana', 2), ('orange', 8), ('apple', 3)].\n",
    "    # Sum values by key and sort by value descending.\n",
    "\n",
    "data = [('apple',5), ('banana',2), ('orange',8), ('apple',3)]\n",
    "rdd = sc.parallelize(data)                        # Create RDD of key-value pairs\n",
    "\n",
    "summed_rdd = rdd.reduceByKey(lambda a, b: a + b)  # Sum values by key\n",
    "sorted_rdd = summed_rdd.sortBy(lambda x: x[1], ascending=False)  # Sort by value descending\n",
    "\n",
    "print(sorted_rdd.collect())  # Collect sorted RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7eb6af0-17bb-4e04-a296-7e3511d03bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in 2020: 90982\n"
     ]
    }
   ],
   "source": [
    "##11 Load covid-dataset/covid-data.csv, filter for 2020, and count records.\n",
    "\n",
    "\n",
    "rdd = sc.textFile(\"covid-dataset/covid-data.csv\")  # Load CSV\n",
    "header = rdd.first()                               # Get header\n",
    "data_2020 = rdd.filter(lambda row: row != header and row.split(',')[3].startswith('2020'))  # Filter 2020\n",
    "\n",
    "count = data_2020.count()  # Count rows\n",
    "\n",
    "print(f\"Number of records in 2020: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24c0a263-6f33-41f9-9b5f-3ecdb74833e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('World', 775866783.0), ('High-income countries', 429044049.0), ('Asia', 301499099.0), ('Europe', 252916868.0), ('Upper-middle-income countries', 251753518.0)]\n"
     ]
    }
   ],
   "source": [
    "##12 Load covid-dataset/covid-data.csv, find max total_cases per country, and get top 5.\n",
    "\n",
    "\n",
    "rdd = sc.textFile(\"covid-dataset/covid-data.csv\")  # Load CSV\n",
    "header = rdd.first()                               # Get header\n",
    "cases = rdd.filter(lambda row: row != header)     \\\n",
    "           .map(lambda row: (row.split(',')[2], float(row.split(',')[4] or 0.0)))  # (country, total_cases)\n",
    "\n",
    "max_cases = cases.reduceByKey(lambda a, b: a if a > b else b)  # Max cases per country\n",
    "top_5 = max_cases.top(5, key=lambda x: x[1])                   # Top 5 countries by cases\n",
    "\n",
    "print(top_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33a6b5a7-63af-40df-b1af-d2774adb31c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('is', 2), ('spark', 2), ('fast', 1), ('awesome', 1)]\n"
     ]
    }
   ],
   "source": [
    "##13 Create RDDs from 'spark is awesome and spark is fast' and count word frequencies.\n",
    "\n",
    "\n",
    "text1 = sc.parallelize(\"spark is awesome\".split())  # RDD from first text\n",
    "text2 = sc.parallelize(\"spark is fast\".split())     # RDD from second text\n",
    "\n",
    "combined_rdd = text1.union(text2)                   # Combine both RDDs\n",
    "word_counts = combined_rdd.map(lambda w: (w, 1))   \\\n",
    "                          .reduceByKey(lambda a, b: a + b)  # Count occurrences\n",
    "\n",
    "print(word_counts.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5137f3cb-8af4-4a5b-ae9a-538d405790c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing or non-numeric new_cases: 19276\n"
     ]
    }
   ],
   "source": [
    "##14 Load covid-dataset/covid-data.csv, count rows with empty or non-numeric new_cases.\n",
    "\n",
    "\n",
    "rdd = sc.textFile(\"covid-dataset/covid-data.csv\")  # Load CSV\n",
    "header = rdd.first()                               # Get header\n",
    "\n",
    "# Filter rows with empty or non-numeric 'new_cases' (6th column)\n",
    "invalid_cases = rdd.filter(lambda row: row != header) \\\n",
    "                   .filter(lambda row: not row.split(',')[5] or not row.split(',')[5].replace('.', '').replace('-', '').isdigit())\n",
    "\n",
    "count = invalid_cases.count()  # Count invalid rows\n",
    "\n",
    "print(f\"Number of rows with missing or non-numeric new_cases: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff236b57-bbb0-40a1-9f93-2055509cf817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'are', 'Spark', 'uses', 'powerful', 'fun', 'RDDs']\n"
     ]
    }
   ],
   "source": [
    "##15 Create an RDD from [\"Spark is fun\", \"RDDs are powerful\", \"Spark uses RDDs\"], get distinct words.\n",
    "\n",
    "sentences = [\"Spark is fun\", \"RDDs are powerful\", \"Spark uses RDDs\"]\n",
    "rdd = sc.parallelize(sentences)              # Create RDD from list of sentences\n",
    "words = rdd.flatMap(lambda s: s.split()).distinct()    # Split sentences into words and get unique words\n",
    "\n",
    "print(words.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "baded9af-ad8b-4aff-8837-d00c0d97a501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 55, Count: 10\n"
     ]
    }
   ],
   "source": [
    "##16 Create an RDD from [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], compute sum and count.\n",
    "\n",
    "\n",
    "numbers = [1,2,3,4,5,6,7,8,9,10]\n",
    "rdd = sc.parallelize(numbers)  \n",
    "\n",
    "# Aggregate to compute sum and count in one pass\n",
    "result = rdd.aggregate(\n",
    "    (0,0),                                           # Initial value (sum, count)\n",
    "    lambda acc, v: (acc[0]+v, acc[1]+1),            # SeqOp: update sum & count per partition\n",
    "    lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])  # CombOp: merge partitions\n",
    ")\n",
    "\n",
    "print(f\"Sum: {result[0]}, Count: {result[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23c29405-c57e-4603-88aa-bfacb6272ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', ([2], ['y'])), ('c', ([], ['z'])), ('a', ([1, 3], ['x']))]\n"
     ]
    }
   ],
   "source": [
    "##17 Create RDDs: [('a', 1), ('b', 2), ('a', 3)] and [('a', 'x'), ('b', 'y'), ('c', 'z')], use cogroup.\n",
    "\n",
    "\n",
    "rdd1 = sc.parallelize([('a',1), ('b',2), ('a',3)])   # First RDD\n",
    "rdd2 = sc.parallelize([('a','x'), ('b','y'), ('c','z')])  # Second RDD\n",
    "\n",
    "cogrouped = rdd1.cogroup(rdd2)                       # Group values by key from both RDDs\n",
    "result = cogrouped.mapValues(lambda x: (list(x[0]), list(x[1])))  # Convert iterables to lists\n",
    "\n",
    "print(result.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2806d1e2-8521-4580-88f3-3a5bd0110bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled records count: 4217\n"
     ]
    }
   ],
   "source": [
    "##18 Load covid-dataset/covid-data.csv, take 1% sample with replacement, count records.\n",
    "\n",
    "\n",
    "\n",
    "rdd = sc.textFile(\"covid-dataset/covid-data.csv\")            # Load CSV\n",
    "header = rdd.first()                             # Get header\n",
    "data = rdd.filter(lambda row: row != header)    # Skip header\n",
    "\n",
    "sampled = data.sample(True, 0.01)               # Take 1% sample with replacement\n",
    "count = sampled.count()                          # Count sampled rows\n",
    "\n",
    "print(f\"Sampled records count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d53684f-823a-4801-a9a2-5cdd399d5c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b'), (3, 'a'), (3, 'b')]\n"
     ]
    }
   ],
   "source": [
    "##19 Create RDDs: [1, 2, 3] and ['a', 'b'], compute Cartesian product.\n",
    "\n",
    "\n",
    "rdd1 = sc.parallelize([1, 2, 3])      # First RDD\n",
    "rdd2 = sc.parallelize(['a', 'b'])    # Second RDD\n",
    "\n",
    "cartesian_rdd = rdd1.cartesian(rdd2)  # Compute Cartesian product of two RDDs\n",
    "\n",
    "print(cartesian_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a54bb424-8c33-43c6-b096-db0a0135b051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', '1'), ('a', '2'), ('a', '3'), ('b', '4'), ('b', '5')]\n"
     ]
    }
   ],
   "source": [
    "##20 Create an RDD from [('a', '1 2 3'), ('b', '4 5')], split values with flatMapValues.\n",
    "\n",
    "\n",
    "data = [('a','1 2 3'), ('b','4 5')]\n",
    "rdd = sc.parallelize(data)                             # Create RDD of key-value pairs\n",
    "\n",
    "split_rdd = rdd.flatMapValues(lambda v: v.split())     # Split values into multiple key-value pairs\n",
    "\n",
    "print(split_rdd.collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
